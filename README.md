# LLMRed
Safe, redacted LLM red-team PoCs. Researching UX-level vulnerabilities, multi-agent exploits &amp; state persistence.



LLM Safety Lab
Safe, redacted AI red-teaming case studies

About
This repository documents safe, redacted proofs-of-concept (PoCs) from AI red-teaming research.
Focus areas include UX-level vulnerabilities, multi-agent delegation, capability escalation, and state persistence in large language models.

All findings are:

Performed with benign payloads

Responsibly disclosed to vendors before publication

Redacted to prevent misuse

Contents
N3 â€“ Multi-Agent Indirect Shell Delegation (Redacted)
Demonstrates benign shell command execution and persistent state across turns via agent role hierarchy.

Safety Notice
This repository contains only safe, non-reproducible examples.
Exact prompts, payloads, and unsafe methods are withheld to prevent malicious use.
For full technical details, private review can be arranged with appropriate safeguards.

License & Ethics
This project is licensed under the MIT License with an added responsible-use clause.
By accessing this repository, you agree not to use any material herein for unsafe or malicious purposes.
